
\chapter{Introduction}
\label{ch:introduction}

Robotic manipulation of objects is an increasing field of research which has struggled \DMcomment{sounds weird} researches from many years ago. In several industrial environments robots can be easily programmed \DMC{considering}{when} the objects \DMC{to be}{are} known a priori, i.e. the manipulation is always the same, and \DMC{usually}{} robot operations avoid \DMC{to deal in}{} cluttered scene\DM{s}, but th\DMC{is situation needs a}{e} workspace \DM{has to be} designed in a manner to provide to the robot a non cluttered scene. \DMC{But}{However,} there are situations in which robots with enhanced intelligence can be useful.
 An example in which a robot could face a cluttered scenario is the one of the \textit{Amazon Picking Challenge} \citep{APC}, which \DMC{has the objective to provide}{provides} a challenge problem to the robotics research community that involves integrating the state of the art in object perception, motion planning, grasp planning, and task planning to manipulate real-world items in industrial settings such \DM{the} ones human operators face in Amazon's warehouse. Joey Durham from Amazon Robotics describes the challenges of this competition \DMC{with the following sentence}{as follows}:
\begin{displayquote}
 “A robot for picking purposes must possess a lot of skills: The selected item must be identified, handled and deposited correctly and safely. This requires a certain level of visual, tactile and haptic perceptive skills and great handling capabilities.”
\end{displayquote}

In this thesis we \DMC{are going to}{} investigate a simple approach for a complex manipulation problem. Our approach tries to replicate \DMC{the} human reasoning during the manipulation of cluttered objects in table clearing tasks. The objective is removing all the objects on\DMC{to}{} a table\DMC{ considering that their poses could limit the manipulation actions}{}. 

This thesis has been developed in the \textbf{Institut de Robòtica i Informàtica Industrial} (IRI) in the  Perception and Manipulation laboratory with the supervision of \textit{Guillem Alenyá Ribas} as director and \textit{David Martinéz Martinéz} as co-director. 

\iffalse
This chapter is structured as follow, first an introduction to the problem and the approach used is commented, then a review of the state of the art regarding manipulation planning is presented to the reader and final the experimental set up is presented since it will be useful to understand some techniques and choice for the algorithm which will be discussed in the next chapters. 
\fi

This work is structured as follow. First an introduction to the problem \DM{that} we want to solve is presented in \DMC{the}{} Chapter \ref{ch:introduction}. Then, in Chapter \ref{ch:state_of_the_art} a review of the current state of the art in manipulation planning is done. The planner developed will be \DMC{commented}{explained?} in Chapter \ref{ch:task_planner}\DM{,} and the \DMC{implemented algorithm to compute the predicates and control the robot will be commented}{cambiar} in Chapter \ref{ch:algorithm}. In Chapter \ref{ch:implementation} the software\DMC{s}{ design} \DMC{employed for the implementation of the code}{} will be presented as well the algorithm's structure. Finally \DMC{some}{} experiments and conclusions about this work are discussed in Chapter \ref{ch:experiments} and \ref{ch:conclusions} respectively.

\DMC{In this chapter first an introduction to the problem and the approach adopted is commented}{This chapter introduces...}\DMC{, then the experimental set up is presented since it will be useful to understand some techniques and choices for the algorithm which will be discussed in next chapters.}{ and the problem that we are tackling}. 

\section{Problem Approach}
In this section the approach to solve the planning problem is described. The strategy to solve the problem is inspired \DMC{to a human-like solution}{by the way humans solve it}. A human would \DMC{solve such task with}{use} \DMC{three main}{mainly three} actions: grasp an object, push it or drag it. When it is not possible to grasp an object, because other objects hinder the human to put the hands in the proper way to grasp the desired one, he/she \DMC{has to interact}{interacts} with the scene \DMC{in order}{} to \DMC{put the desired object in a pose where it can be grasped}{make the object graspable}.

Dragging is a very complex action which \DMC{will require}{requires} the robot to lay the end effector on the top of an object. This is a very hard action which would imply an implementation of a reliable controller, moreover the goal is not only to move the object but also not to ruin it.\DMcomment{I don't understand the difference between pushing and dragging...}

\DM{The} Pushing action is \DMC{an action}{} easier than dragging since it only requires to put the end effector in a certain pose and then push the object by moving the end effector. However\DMC{ the pushing action is complex since to push an object making it}{, it is difficult to push an object and ensure that it follows the desired path...} following a certain path the pushing pose has to be chosen taking into account the shape of the object and a controller would be needed in order to correct the pose along the path. \DMC{Despite this we handled to perform that action in a easy way by considering only objects with basic shapes.}{We assumed that all objects had basic shapes, so that a simple pushing action performs well.}

\DMC{For}{Based on} these considerations\DM{,} the \DMC{main}{} actions the robot has to use \DMC{in order to interact with the objects}{} are grasping and pushing.
Grasping is the most important action since it lets to take an object \DMC{from the pile of objects}{} and drop it somewhere, for instance into a bin, clearing in this way the table. There exist different works facing the same task by focusing only in \DM{the} grasping \DM{skill} \citep{haf,AGILE}. The pushing \DMC{becomes useful considering the problem that}{is useful when} two adjacent objects could not be grasped if they are so close such that the robot's gripper, when attempting to grasp an object, is going to collide with the adjacent one, making the object \DMC{of interest}{} ungraspable. \DMC{From this consideration is necessary the pushing action, in order to}{The pushing action can} separate adjacent objects \DMC{which could}{that are} mutually \DMC{exclude}{excluding} themselves \DMC{to be}{from being} grasped. 

\DMC{Being observant to the philosophy of a human-inspired solution, the manipulation of the objects is intended in manner to avoid to interact with more than one object per action. That is, when we push, or grasp, an object only that object will change its pose. }{For simplicity we only consider actions that interact with a single object.} \DMcomment{For example, humans may push several objects together in some cases...}

The robot's \DMC{intelligence is enhanced by}{decision maker uses} a planning system  \DMC{which will return}{that returns} a sequence of actions \DMC{in order to}{that} achieve the goal \DMC{to}{of} clear\DM{ing} the table. 
The robot \DMC{will reason totally}{reasons} in an abstraction level by considering only symbolic predicates which \DMC{will be}{are} obtained by hand-built mapping functions making \DMC{simpler}{} the reasoning step \DM{simpler}. It is important noting that this problem should includes \DMC{important}{relevant} geometric information which are very useful for the planner, such as how much to push an object, in what direction and the inverse kinematic of the robot. \DMC{This}{} geometric information \DMC{are}{is} translated into symbolic \DMC{ones relaxing the planning phase}{literals}, although these are not able to catch all the geometric information. Then through backtracking the robot \DMC{will} evaluate if the returned plan can be actually executed by the robot (i.e. evaluate if the action to execute involves the robot to move inside its working space), if not it updates the predicates and replans. \DMcomment{This paragraph is too complicated. Just say that the robot reasons in a symbolic level (planning) with limited geometrical information. Then the plan is checked to see if it is feasible, and if it isn't, backtracking is done.}
We assume the world is perfectly known\DM{.} \DMC{and the actions are not reliable. To overcome this}{As actions are actually non-reliable,} the planner replans after the execution of each action.  
\DMC{To detect the objects the Microsoft Kinect sensor is used as vision system and a geometry based segmentation is used to segment the objects in the scene.}{I think it doesn't fit in this paragraph, you can explain it later.}

\section{Set Up}
\DMC{In order to make easier the understanding of the approach used,}{} the set up of the environment the robot will work in is \DMC{here presented}{presented here}. 

The robot used is a Barret WAM arm, which is a 7 degree of freedom (DoF) manipulator arm \DMC{shown in }{(}Figure \ref{fig:wam_1}\DM{)}. 
\begin{figure}[htp]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[height=4cm]{Img/set_up/wam.jpg}
\caption{Barrett WAM arm}\label{fig:wam_1}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=5cm]{Img/set_up/Kinect.jpg}
\caption{Microsoft Kinect sensor}\label{fig:kinect}
\end{subfigure}
\caption{Robot and vision sensor.}
\end{figure}
%This robot is characterized by a low inertia of the end effector thanks to the kind of its actuators. 
The WAM is noteworthy because it does not use any gears for manipulating the joints, but cable drives, so there are no backlash problems and it is both fast and stiff. Cable drives permit low friction and ripple-free torque transmission from the actuator to the joints. 
To detect the objects a Kinect camera, a RGB-D sensor, is employed (\figref{fig:kinect}).

To manipulate the objects \DMC{the manipulation skills of}{} the robot \DMC{are enhanced by}{has} a gripper designed in the IRI institute and actuated by Dynamixel motors. Such a gripper is depicted in Figure \ref{fig:gripper_general} from several point of views. Its closing width\footnote{Distance between the fingers when the gripper is closed.} is $3$ centimetres while \DMC{the}{its} opening width\footnote{Distance between the fingers when the gripper is open.} is of $8$ centimetres, therefore we are constrained to grasp objects with a width in the range $[3 \div 8]cm$.

\begin{figure}[htp]
\centering
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[height=3cm]{Img/set_up/gripper1.png}
%\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[height=3cm]{Img/set_up/gripper2.png}
%\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\centering
\includegraphics[width=3cm]{Img/set_up/gripper3.png}
%\caption{}
\end{subfigure}
\caption{Gripper used for the experiments}\label{fig:gripper_general}
\end{figure}

For the task planner, as the reader will see in Chapters \ref{ch:task_planner} and \ref{ch:algorithm}, the model of the gripper will be an important resource 
in order to compute the predicates. 

The gripper will be modeled measuring some principal elements such as: finger's width and height, gripper's width, height and deep, closing and opening width. The modeling procedure is depicted in Figure \ref{fig:gripper_modelling}. The resulting model is a simple triangle mesh which includes all the important geometric information of the gripper.
\begin{figure}[h]
\centering
\includegraphics[width=5.0cm]{Img/set_up/wam_gripper2.png}
\caption{Gripper and WAM.}\label{fig:wam_gripper}
\end{figure}
Such a simple model allows the collision algorithm commented in Chapter \ref{ch:algorithm} to check for collision in just \DM{a} few millisecond\DM{s}. 
A more detailed and complex model would \DMC{only lead as benefit a}{have} higher precision, but \DMC{for this kind of task an extremely accurate precision}{such a high accuracy} is not needed, and it \DMC{will}{would} slow down the algorithm\DMC{ because the collision checking procedure would be more computationally expensive}{}. 
The gripper \DMC{will be}{is} mounted in the end effector of the robot as shown in Figure \ref{fig:wam_gripper}. 


\begin{figure}[htp]
\centering
\begin{subfigure}[t]{0.25\textwidth}
\centering
\stackunder[5pt]	{\includegraphics[height=3cm]{Img/set_up/gripper_modelling1.png}}{Elements measured}
\end{subfigure} 
\begin{subfigure}[t]{0.3\textwidth}
\centering
\stackunder[5pt]	{\includegraphics[height=3cm]{Img/set_up/gripper_open_model.png}}{Opened gripper mesh model}
\end{subfigure}
\hspace{1cm}
\begin{subfigure}[t]{0.3\textwidth}
\centering
\stackunder[5pt]	{\includegraphics[height=3cm]{Img/set_up/gripper_closed_model.png}}{Closed gripper mesh model}
%\caption{}
\end{subfigure}
\caption{At the left the principal elements measured are highlighted for the opened gripper model. The gripper mesh model is here shown in the PCL visualizer. The \textcolor{red}{red}, \textcolor{green}{green} and \textcolor{blue}{blue} axis are respectively the x, y and z axis. }\label{fig:gripper_modelling}
\end{figure}

The scenario the robot is going to work in is composed of a table and the objects will lay on top of it. In Figure \ref{fig:setup_} the main elements of the set up are highlighted. The WAM arm's base is in a fixed position with respect the table and the Kinect camera is located on top of the table pointing down. The Kinect is calibrated and the fixed transformation between the Kinect's frames and the base frame of the robot is known, so all the points measured by the Kinect can be expressed in coordinates with respect the robot's base frame. An example of a cluttered scene the robot is going to deal with \DMC{in this thesis}{} is depicted in Figure \ref{fig:example_scene}, and the same scene seen by the Kinect is shown in Figure \ref{fig:kinect_view}. \DMcomment{Maybe say it in the opposite way: Figure X shows an example of a cluttered scene, and Fig. Y shows that same scene as seen from the Kinect camera.} \DMC{With respect the kinect's view the robot is located at the top side of the image, outside the field of view of the Kinect, in order to avoid to detect the arm as an object. If the robot arm would be present in the depth image we should apply some algorithms to identify the robot arm, which could also produce important occlusions, that is hide some objects from the Kinect's view. In order to avoid all this, the depth images considered are the ones provided by the Kinect when the robot is no more in the Kinect's view (these poses are known a priori). }{To avoid occlusions, we wait until the robot finishes to execute an action and moves away before taking new images. (simple and shorter is better)}

\begin{figure}[htp]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[height=7.5cm]{Img/set_up/set_up_nice2.png}
\caption{Principal elements of the experimental set up.}\label{fig:setup_}
\end{subfigure}
\qquad \qquad 
\begin{subfigure}[b]{0.4\textwidth}
\centering
\includegraphics[height=3cm]{Img/set_up/example_setup.jpg}
\caption{Example of a cluttered scene the robot is going to work with.}\label{fig:example_scene}
\vspace{2ex}
\includegraphics[height=3cm]{Img/set_up/view_kinect.png}
\caption{Kinect's view.}\label{fig:kinect_view}
\end{subfigure}
\caption{Experimental set up\DMC{, and}{ with} an example of a cluttered scene the robot is going to interact with.}
\label{fig:setup}
\end{figure}


%Concerning the example of Figure \ref{fig:example_scene} it can be appreciated that the manipulation needed in order to clean all the table with carefulness is quite complex. All the sequence of actions will be retrieved by the planner.



